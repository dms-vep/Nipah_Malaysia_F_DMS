{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bdc2a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this notebook finds candidate stabilizing space-filling mutations using cell entry DMS data and cavity data generated from pyKVFinder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "402ebcd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import tomllib\n",
    "import pandas as pd\n",
    "import tomllib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a98b46d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "h_bond_sites = [\n",
    "    30,\n",
    "    31,\n",
    "    44,\n",
    "    45,\n",
    "    46,\n",
    "    50,\n",
    "    56,\n",
    "    64,\n",
    "    66,\n",
    "    82,\n",
    "    84,\n",
    "    88,\n",
    "    98,\n",
    "    103,\n",
    "    135,\n",
    "    138,\n",
    "    139,\n",
    "    140,\n",
    "    143,\n",
    "    146,\n",
    "    148,\n",
    "    152,\n",
    "    156,\n",
    "    163,\n",
    "    164,\n",
    "    166,\n",
    "    167,\n",
    "    177,\n",
    "    178,\n",
    "    181,\n",
    "    182,\n",
    "    189,\n",
    "    193,\n",
    "    194,\n",
    "    196,\n",
    "    198,\n",
    "    200,\n",
    "    204,\n",
    "    208,\n",
    "    217,\n",
    "    219,\n",
    "    224,\n",
    "    238,\n",
    "    240,\n",
    "    244,\n",
    "    248,\n",
    "    254,\n",
    "    258,\n",
    "    259,\n",
    "    261,\n",
    "    265,\n",
    "    270,\n",
    "    273,\n",
    "    275,\n",
    "    279,\n",
    "    281,\n",
    "    286,\n",
    "    287,\n",
    "    294,\n",
    "    295,\n",
    "    300,\n",
    "    307,\n",
    "    325,\n",
    "    336,\n",
    "    341,\n",
    "    342,\n",
    "    343,\n",
    "    344,\n",
    "    350,\n",
    "    351,\n",
    "    353,\n",
    "    354,\n",
    "    359,\n",
    "    362,\n",
    "    366,\n",
    "    372,\n",
    "    375,\n",
    "    386,\n",
    "    389,\n",
    "    394,\n",
    "    395,\n",
    "    396,\n",
    "    413,\n",
    "    415,\n",
    "    432,\n",
    "    451,\n",
    "    455,\n",
    "    466,\n",
    "    468,\n",
    "    471,\n",
    "]\n",
    "\n",
    "salt_bridge_sites = [44, 45, 82, 136, 139, 240, 287, 295]\n",
    "helix_sites = [\n",
    "    29,\n",
    "    30,\n",
    "    31,\n",
    "    32,\n",
    "    33,\n",
    "    34,\n",
    "    35,\n",
    "    65,\n",
    "    66,\n",
    "    67,\n",
    "    68,\n",
    "    69,\n",
    "    74,\n",
    "    75,\n",
    "    76,\n",
    "    77,\n",
    "    78,\n",
    "    79,\n",
    "    80,\n",
    "    81,\n",
    "    82,\n",
    "    83,\n",
    "    84,\n",
    "    85,\n",
    "    86,\n",
    "    87,\n",
    "    88,\n",
    "    89,\n",
    "    90,\n",
    "    91,\n",
    "    92,\n",
    "    93,\n",
    "    94,\n",
    "    95,\n",
    "    96,\n",
    "    97,\n",
    "    98,\n",
    "    99,\n",
    "    117,\n",
    "    118,\n",
    "    119,\n",
    "    120,\n",
    "    121,\n",
    "    125,\n",
    "    126,\n",
    "    127,\n",
    "    128,\n",
    "    129,\n",
    "    130,\n",
    "    131,\n",
    "    132,\n",
    "    133,\n",
    "    134,\n",
    "    135,\n",
    "    136,\n",
    "    137,\n",
    "    138,\n",
    "    139,\n",
    "    140,\n",
    "    141,\n",
    "    142,\n",
    "    143,\n",
    "    144,\n",
    "    145,\n",
    "    146,\n",
    "    147,\n",
    "    148,\n",
    "    149,\n",
    "    150,\n",
    "    151,\n",
    "    152,\n",
    "    153,\n",
    "    154,\n",
    "    175,\n",
    "    176,\n",
    "    177,\n",
    "    178,\n",
    "    179,\n",
    "    180,\n",
    "    181,\n",
    "    182,\n",
    "    183,\n",
    "    184,\n",
    "    185,\n",
    "    186,\n",
    "    187,\n",
    "    193,\n",
    "    194,\n",
    "    195,\n",
    "    196,\n",
    "    197,\n",
    "    198,\n",
    "    199,\n",
    "    200,\n",
    "    201,\n",
    "    202,\n",
    "    203,\n",
    "    204,\n",
    "    205,\n",
    "    206,\n",
    "    207,\n",
    "    208,\n",
    "    209,\n",
    "    210,\n",
    "    211,\n",
    "    212,\n",
    "    213,\n",
    "    214,\n",
    "    215,\n",
    "    228,\n",
    "    229,\n",
    "    230,\n",
    "    231,\n",
    "    232,\n",
    "    233,\n",
    "    234,\n",
    "    235,\n",
    "    236,\n",
    "    238,\n",
    "    239,\n",
    "    240,\n",
    "    241,\n",
    "    242,\n",
    "    243,\n",
    "    244,\n",
    "    245,\n",
    "    252,\n",
    "    253,\n",
    "    254,\n",
    "    255,\n",
    "    256,\n",
    "    257,\n",
    "    258,\n",
    "    259,\n",
    "    328,\n",
    "    329,\n",
    "    330,\n",
    "    331,\n",
    "    349,\n",
    "    350,\n",
    "    351,\n",
    "    352,\n",
    "    353,\n",
    "    354,\n",
    "    355,\n",
    "    356,\n",
    "    357,\n",
    "    358,\n",
    "    359,\n",
    "    360,\n",
    "    361,\n",
    "    362,\n",
    "    363,\n",
    "    437,\n",
    "    438,\n",
    "    439,\n",
    "    440,\n",
    "    441,\n",
    "    442,\n",
    "    452,\n",
    "    453,\n",
    "    454,\n",
    "    455,\n",
    "    456,\n",
    "    457,\n",
    "    458,\n",
    "    459,\n",
    "    460,\n",
    "    461,\n",
    "    462,\n",
    "    463,\n",
    "    464,\n",
    "    465,\n",
    "    466,\n",
    "    467,\n",
    "    468,\n",
    "    469,\n",
    "    470,\n",
    "    471,\n",
    "    472,\n",
    "    473,\n",
    "    474,\n",
    "    475,\n",
    "    476,\n",
    "    477,\n",
    "    478,\n",
    "    479,\n",
    "    480,\n",
    "    481,\n",
    "    482,\n",
    "]\n",
    "\n",
    "sheet_sites = [\n",
    "    38,\n",
    "    39,\n",
    "    40,\n",
    "    41,\n",
    "    42,\n",
    "    43,\n",
    "    44,\n",
    "    45,\n",
    "    46,\n",
    "    47,\n",
    "    48,\n",
    "    53,\n",
    "    54,\n",
    "    55,\n",
    "    56,\n",
    "    57,\n",
    "    58,\n",
    "    59,\n",
    "    60,\n",
    "    101,\n",
    "    102,\n",
    "    103,\n",
    "    113,\n",
    "    114,\n",
    "    115,\n",
    "    116,\n",
    "    122,\n",
    "    123,\n",
    "    124,\n",
    "    158,\n",
    "    159,\n",
    "    160,\n",
    "    161,\n",
    "    169,\n",
    "    170,\n",
    "    171,\n",
    "    172,\n",
    "    173,\n",
    "    226,\n",
    "    227,\n",
    "    263,\n",
    "    264,\n",
    "    265,\n",
    "    266,\n",
    "    267,\n",
    "    268,\n",
    "    269,\n",
    "    270,\n",
    "    275,\n",
    "    276,\n",
    "    277,\n",
    "    278,\n",
    "    279,\n",
    "    280,\n",
    "    281,\n",
    "    282,\n",
    "    286,\n",
    "    287,\n",
    "    288,\n",
    "    289,\n",
    "    290,\n",
    "    291,\n",
    "    292,\n",
    "    293,\n",
    "    294,\n",
    "    295,\n",
    "    296,\n",
    "    297,\n",
    "    298,\n",
    "    301,\n",
    "    302,\n",
    "    307,\n",
    "    308,\n",
    "    309,\n",
    "    310,\n",
    "    315,\n",
    "    316,\n",
    "    317,\n",
    "    318,\n",
    "    319,\n",
    "    322,\n",
    "    323,\n",
    "    324,\n",
    "    325,\n",
    "    326,\n",
    "    332,\n",
    "    333,\n",
    "    337,\n",
    "    338,\n",
    "    339,\n",
    "    340,\n",
    "    345,\n",
    "    346,\n",
    "    365,\n",
    "    366,\n",
    "    367,\n",
    "    376,\n",
    "    377,\n",
    "    378,\n",
    "    379,\n",
    "    382,\n",
    "    383,\n",
    "    384,\n",
    "    385,\n",
    "    392,\n",
    "    393,\n",
    "    394,\n",
    "    410,\n",
    "    411,\n",
    "    412,\n",
    "    419,\n",
    "    420,\n",
    "    421,\n",
    "    422,\n",
    "    425,\n",
    "    426,\n",
    "    427,\n",
    "    428,\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3726ffe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input files\n",
    "entry_df = pd.read_csv(\n",
    "    \"../../results/filtered_data/cell_entry/Nipah_F_func_effects_filtered.csv\"\n",
    ").drop(columns=[\"effect_std\", \"n_selections\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b169f883",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the TOML file\n",
    "with open(\"../../data/stabilizing_data/5EVM_cavities_output.toml\", \"rb\") as f:\n",
    "    data = tomllib.load(f)\n",
    "\n",
    "# Extract the RESULTS section\n",
    "results = data['RESULTS']\n",
    "\n",
    "# Get all the cavity IDs (KAA, KAB, etc.)\n",
    "cavity_ids = list(results['RESIDUES'].keys())\n",
    "\n",
    "# Create lists to store the data\n",
    "rows = []\n",
    "\n",
    "# Iterate through each cavity\n",
    "for cavity_id in cavity_ids:\n",
    "    # Get the residues for this cavity\n",
    "    residues = results['RESIDUES'][cavity_id]\n",
    "    for residue in residues:\n",
    "        site, chain, residue_type = residue\n",
    "        rows.append({\n",
    "            'cavity_id': cavity_id,\n",
    "            'site': site,\n",
    "            'chain': chain,\n",
    "            'wildtype': residue_type,\n",
    "            'area': results['AREA'][cavity_id],\n",
    "            'volume': results['VOLUME'][cavity_id],\n",
    "            'avg_depth': results['AVG_DEPTH'][cavity_id],\n",
    "            'max_depth': results['MAX_DEPTH'][cavity_id],\n",
    "            'avg_hydropathy': results['AVG_HYDROPATHY'][cavity_id]\n",
    "        })\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(rows)\n",
    "df['site'] = df['site'].astype(int)\n",
    "df = df.query('site <= 482') #filter out any residues that are not in DMS data\n",
    "\n",
    "# add structural annotations\n",
    "df = df.sort_values(\"area\", ascending=False).assign(\n",
    "    h_bond_sites=lambda x: x[\"site\"].isin(h_bond_sites),\n",
    "    salt_bridge_sites=lambda x: x[\"site\"].isin(salt_bridge_sites),\n",
    "    helix_sites=lambda x: x[\"site\"].isin(helix_sites),\n",
    "    sheet_sites=lambda x: x[\"site\"].isin(sheet_sites),\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "display(df.query('area < 20'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5caf0260",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get list of sites with low accessibility, we want to keep these\n",
    "low_accessibility_sites = (\n",
    "    pd.read_csv(\"../../results/residue_accessibility/5evm_accessibility.csv\")\n",
    "    .query(\"mean_accessibility < 10\")[\"site\"]\n",
    "    .tolist()\n",
    ")\n",
    "print(f\"Number of low accessibility sites: {len(low_accessibility_sites)}\")\n",
    "# get list of all cavities in structure\n",
    "cavity_list = df['cavity_id'].unique().astype(str).tolist()\n",
    "\n",
    "empty_list = [] # to store final dfs for each cavity\n",
    "# iterate through each cavity\n",
    "for cavity in cavity_list:\n",
    "    # pull out sites specific for this cavity\n",
    "    subset = df[df['cavity_id'] == cavity].drop_duplicates(subset=['site'])\n",
    "    \n",
    "    # generate list of sites in cavity\n",
    "    sites_subset = sorted(set(subset['site'].unique().tolist()))\n",
    "    \n",
    "    # extract effects for these sites\n",
    "    effect_subset = entry_df[entry_df['site'].isin(sites_subset)]\n",
    "    \n",
    "    # merge effects with cavity subset\n",
    "    merged_subsets = pd.merge(effect_subset, subset, on=['site', 'wildtype'], how='left')\n",
    "\n",
    "    # calculate std deviation of effects for each site in cavity and remove sites with low variation, they are not interesting\n",
    "    high_variation_df = (\n",
    "        merged_subsets.groupby(\"site\")\n",
    "        .agg(\n",
    "            effect_max=(\"effect\", \"max\"),\n",
    "            effect_std=(\"effect\", \"std\"),\n",
    "            effect_min_top3=(\"effect\", lambda x: x.nlargest(3).min()),\n",
    "        )\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    high_variation_sites = high_variation_df.query(\n",
    "        \"effect_std >= 0.5 and effect_min_top3 >= -1\"\n",
    "    )[\"site\"].tolist()\n",
    "    \n",
    "    # remove sites that are involved in H-bonds or salt bridges and only keep sites with high variation in effects\n",
    "    merged_subsets = merged_subsets.assign(is_high_variation_site = merged_subsets['site'].isin(high_variation_sites), low_accessibility_site=merged_subsets['site'].isin(low_accessibility_sites))\n",
    "\n",
    "    \n",
    "    \n",
    "    empty_list.append(merged_subsets)\n",
    "    \n",
    "final_concat = pd.concat(empty_list, ignore_index=True)\n",
    "final_concat = final_concat.drop_duplicates(['site', 'mutant'])\n",
    "#display(final_concat.query('site == 172'))\n",
    "\n",
    "final_concat = final_concat.query('is_high_variation_site and low_accessibility_site')\n",
    "display(final_concat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc17e1f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "entry_df = entry_df.assign(\n",
    "    h_bond_sites=lambda x: x[\"site\"].isin(h_bond_sites),\n",
    "    salt_bridge_sites=lambda x: x[\"site\"].isin(salt_bridge_sites),\n",
    "    helix_sites=lambda x: x[\"site\"].isin(helix_sites),\n",
    "    sheet_sites=lambda x: x[\"site\"].isin(sheet_sites),\n",
    ")\n",
    "\n",
    "merged_entry_acc = entry_df.query(\"site in @low_accessibility_sites\").query('h_bond_sites == False and salt_bridge_sites == False')\n",
    "display(merged_entry_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d011f68b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to assign amino acid type\n",
    "def assign_aa_type(site_num):\n",
    "    if site_num in [\"D\", \"E\"]:\n",
    "        return \"Negative\"\n",
    "    elif site_num in [\"K\", \"R\", \"H\"]:\n",
    "        return \"Positive\"\n",
    "    elif site_num in [\"Q\", \"N\", \"S\", \"T\"]:\n",
    "        return \"Hydrophilic\"\n",
    "    elif site_num in [\"A\", \"I\", \"L\", \"M\", \"V\"]:\n",
    "        return \"Hydrophobic\"\n",
    "    elif site_num in [\"Y\", \"W\", \"F\"]:\n",
    "        return \"Aromatic\"\n",
    "    elif site_num in [\"C\", \"G\", \"P\"]:\n",
    "        return \"Special\"\n",
    "    else:\n",
    "        return \"Other\"\n",
    "\n",
    "\n",
    "final_concat[\"mutant_type\"] = final_concat[\"mutant\"].apply(assign_aa_type)\n",
    "final_concat[\"wildtype_type\"] = final_concat[\"wildtype\"].apply(assign_aa_type)\n",
    "\n",
    "\n",
    "final_concat = final_concat.query('h_bond_sites == False')\n",
    "display(final_concat)\n",
    "\n",
    "\n",
    "grouped_aa_props = final_concat.groupby(['site', 'mutant_type']).agg(\n",
    "    mean_effect = ('effect', 'mean'),\n",
    "    max_effect = ('effect', 'max'),\n",
    "    min_effect = ('effect', 'min'),\n",
    "    n_mutants = ('mutant', 'nunique'),\n",
    "    wildtype = ('wildtype_type', 'first'),\n",
    ").reset_index()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f02eb5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_entry_acc = merged_entry_acc.assign(\n",
    "    mutant_type=lambda x: x[\"mutant\"].apply(assign_aa_type),\n",
    "    wildtype_type=lambda x: x[\"wildtype\"].apply(assign_aa_type),\n",
    ")\n",
    "#display(merged_entry_acc)\n",
    "\n",
    "grouped_aa_entry_all = (\n",
    "    merged_entry_acc.groupby([\"site\", \"mutant_type\"])\n",
    "    .agg(\n",
    "        mean_effect=(\"effect\", \"mean\"),\n",
    "        #max_effect=(\"effect\", \"max\"),\n",
    "        #min_effect=(\"effect\", \"min\"),\n",
    "        n_mutants=(\"mutant\", \"nunique\"),\n",
    "        wildtype=(\"wildtype_type\", \"first\"),\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "display(grouped_aa_entry_all)\n",
    "\n",
    "pivoted = grouped_aa_entry_all.pivot_table(\n",
    "    index=[\"site\", \"wildtype\"],\n",
    "    columns=\"mutant_type\",\n",
    "    values=[\"mean_effect\"],\n",
    ")\n",
    "display(pivoted)\n",
    "\n",
    "sites_meeting_criteria = (\n",
    "    pivoted[\n",
    "        (\n",
    "            (\n",
    "                pivoted[\"mean_effect\"][\"Hydrophobic\"]\n",
    "                < pivoted[\"mean_effect\"][\"Hydrophilic\"]\n",
    "            )\n",
    "            | (\n",
    "                pivoted[\"mean_effect\"][\"Aromatic\"]\n",
    "                < pivoted[\"mean_effect\"][\"Hydrophilic\"]\n",
    "            )\n",
    "        )\n",
    "        & (\n",
    "            (pivoted.index.get_level_values(\"wildtype\") == \"Hydrophilic\")\n",
    "            | (pivoted.index.get_level_values(\"wildtype\") == \"Positive\")\n",
    "            | (pivoted.index.get_level_values(\"wildtype\") == \"Negative\")\n",
    "        )\n",
    "    ]\n",
    "    .index.get_level_values(0)\n",
    "    .tolist()\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"Hydrophilic sites: {sites_meeting_criteria} \\n(n={len(sites_meeting_criteria)})\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbaeb4d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pivoted = grouped_aa_props.pivot_table(\n",
    "    index=[\"site\", \"wildtype\"], columns=\"mutant_type\", values=[\"mean_effect\", \"max_effect\", \"min_effect\"]\n",
    ")\n",
    "display(pivoted)\n",
    "\n",
    "# find sites where aromatic mutations are more deleterious than hydrophobic mutations\n",
    "aromatic_more_deleterious = (\n",
    "    pivoted[\n",
    "        (pivoted[\"mean_effect\"][\"Aromatic\"] < pivoted[\"mean_effect\"][\"Hydrophobic\"])\n",
    "        & (pivoted.index.get_level_values(\"wildtype\") != \"Hydrophilic\")\n",
    "    ]\n",
    "    .index.get_level_values(0)\n",
    "    .tolist()\n",
    ")\n",
    "print(f\"Aromatic more deleterious than hydrophobic: {aromatic_more_deleterious} \\n(n={len(aromatic_more_deleterious)})\")\n",
    "\n",
    "\n",
    "# Find sites where both hydrophobic and aromatic are lower than hydrophilic\n",
    "sites_meeting_criteria = (\n",
    "    pivoted[\n",
    "        ((pivoted[\"mean_effect\"][\"Hydrophobic\"] < pivoted[\"mean_effect\"][\"Hydrophilic\"])\n",
    "        | (pivoted[\"mean_effect\"][\"Aromatic\"] < pivoted[\"mean_effect\"][\"Hydrophilic\"]))\n",
    "        & (\n",
    "            (pivoted.index.get_level_values(\"wildtype\") == \"Hydrophilic\")\n",
    "            | (pivoted.index.get_level_values(\"wildtype\") == \"Positive\")\n",
    "            | (pivoted.index.get_level_values(\"wildtype\") == \"Negative\")\n",
    "        )\n",
    "    ]\n",
    "    .index.get_level_values(0)\n",
    "    .tolist()\n",
    ")\n",
    "\n",
    "print(f\"Hydrophilic sites: {sites_meeting_criteria} \\n(n={len(sites_meeting_criteria)})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e521e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(pivoted.query('site in @sites_meeting_criteria'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f3253ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NipahF_DMS_analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
